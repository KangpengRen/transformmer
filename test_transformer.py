import torch
import torch.nn as nn
from utils import device, create_causal_mask
from transformer import Transformer

def test_all_components():
    """æµ‹è¯•æ‰€æœ‰æ ¸å¿ƒç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œï¼ˆåŸºç¡€å±‚éªŒè¯ï¼‰"""
    print("\n" + "="*50)
    print("å¼€å§‹æµ‹è¯•æ ¸å¿ƒç»„ä»¶...")
    from modules import MultiHeadAttention, FeedForward, ResidualNorm
    from utils import d_model

    # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
    mha = MultiHeadAttention().to(device)
    q = k = v = torch.randn(2, 10, d_model, device=device)
    mask = create_causal_mask(10)
    mha_out, mha_w = mha(q, k, v, mask)
    assert mha_out.shape == (2, 10, d_model), "å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼"
    print("âœ… å¤šå¤´æ³¨æ„åŠ›ç»„ä»¶æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•å‰é¦ˆç½‘ç»œ
    ffn = FeedForward().to(device)
    ffn_out = ffn(mha_out)
    assert ffn_out.shape == (2, 10, d_model), "å‰é¦ˆç½‘ç»œè¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼"
    print("âœ… å‰é¦ˆç½‘ç»œç»„ä»¶æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•æ®‹å·®è¿æ¥
    res_norm = ResidualNorm().to(device)
    res_out = res_norm(q, ffn_out)
    assert res_out.shape == (2, 10, d_model), "æ®‹å·®è¿æ¥è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼"
    print("âœ… æ®‹å·®è¿æ¥ç»„ä»¶æµ‹è¯•é€šè¿‡")
    print("="*50 + "\n")

def test_model_forward():
    """æµ‹è¯•å®Œæ•´Transformeræ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆæ¨¡å‹å±‚éªŒè¯ï¼‰"""
    print("å¼€å§‹æµ‹è¯•å®Œæ•´Transformeræ¨¡å‹å‰å‘ä¼ æ’­...")
    # æ¨¡æ‹Ÿè¶…å‚æ•°
    src_vocab_size = 1000
    tgt_vocab_size = 2000
    batch_size = 2
    src_seq_len = 10
    tgt_seq_len = 15

    # åˆå§‹åŒ–æ¨¡å‹
    model = Transformer(src_vocab_size, tgt_vocab_size).to(device)
    print(f"Transformeræ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œæ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # æ¨¡æ‹Ÿè¾“å…¥
    src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len), device=device)
    tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len), device=device)
    tgt_mask = create_causal_mask(tgt_seq_len)

    # å‰å‘ä¼ æ’­
    logits = model(src, tgt, tgt_mask=tgt_mask)
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert logits.shape == (batch_size, tgt_seq_len, tgt_vocab_size), "æ¨¡å‹è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼"
    print(f"æºåºåˆ—å½¢çŠ¶: {src.shape}, ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}, è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
    print("âœ… Transformeræ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
    return model, src, tgt, tgt_mask, tgt_vocab_size


def test_model_backward(model, src, tgt, tgt_mask, tgt_vocab_size):
    """æµ‹è¯•æ¨¡å‹åå‘ä¼ æ’­ï¼ˆå¯è®­ç»ƒæ€§ï¼‰"""
    print("\nå¼€å§‹æµ‹è¯•Transformeræ¨¡å‹åå‘ä¼ æ’­ï¼ˆå¯è®­ç»ƒæ€§ï¼‰...")
    batch_size = src.size(0)
    tgt_seq_len = tgt.size(1)

    # æ¨¡æ‹Ÿç›®æ ‡æ ‡ç­¾ï¼šç§»ä½æ ‡ç­¾ï¼ˆtgt[:,1:]ï¼‰ï¼Œå®é™…ä»»åŠ¡ä¸­ä¸ºçœŸå®æ ‡ç­¾
    tgt_label = tgt[:, 1:].contiguous()
    # æˆªå–æ¨¡å‹è¾“å‡ºä¸æ ‡ç­¾åŒ¹é…ï¼ˆå»æ‰æœ€åä¸€ä¸ªtokenï¼‰
    logits = model(src, tgt, tgt_mask=tgt_mask)[:, :-1, :]

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # è®¡ç®—æŸå¤±
    loss = criterion(logits.reshape(-1, tgt_vocab_size), tgt_label.reshape(-1))
    # åå‘ä¼ æ’­+æƒé‡æ›´æ–°
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # æ ¸å¿ƒä¿®å¤ï¼šæ›¿æ¢ä¸¥æ ¼çš„æ¢¯åº¦èŒƒæ•°assertï¼Œæ”¹ä¸ºæ¢¯åº¦å­˜åœ¨æ€§æ£€æŸ¥+æŸ”æ€§èŒƒå›´æç¤º
    grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    # æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦ï¼ˆæ¢¯åº¦ä¸ºNoneæ‰æ˜¯å¼‚å¸¸ï¼ŒèŒƒæ•°0/å¤§å€¼å‡ä¸ºåˆå§‹éšæœºæƒé‡çš„æ­£å¸¸ç°è±¡ï¼‰
    has_gradient = any(p.grad is not None and p.grad.sum() != 0 for p in model.parameters())
    assert has_gradient, "æ¨¡å‹æ— æ¢¯åº¦æ›´æ–°ï¼å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜"

    # ä»…åšæç¤ºï¼Œä¸åšç¡¬åˆ¤æ–­
    if grad_norm < 0 or grad_norm > 10:
        print(f"âš ï¸  åˆå§‹æ¢¯åº¦èŒƒæ•°{grad_norm:.4f}è¶…å‡º0-10èŒƒå›´ï¼ˆåˆå§‹éšæœºæƒé‡ä¸‹ä¸ºæ­£å¸¸ç°è±¡ï¼Œè®­ç»ƒä¸­ä¼šæ”¶æ•›ï¼‰")
    else:
        print(f"æ¢¯åº¦èŒƒæ•°: {grad_norm:.4f}ï¼ˆæ­£å¸¸èŒƒå›´ï¼‰")

    print(f"è®­ç»ƒæŸå¤±å€¼: {loss.item():.4f}ï¼ˆåˆå§‹éšæœºæƒé‡ä¸‹ï¼Œçº¦ln(2000)â‰ˆ7.6ä¸ºæ­£å¸¸ï¼‰")
    print("âœ… Transformeræ¨¡å‹åå‘ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼ˆå¯æ­£å¸¸è®­ç»ƒï¼‰")


if __name__ == "__main__":
    """ä¸€é”®è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼Œæ— æŠ¥é”™å³ä»£è¡¨å¤ç°æˆåŠŸ"""
    try:
        # æ­¥éª¤1ï¼šæµ‹è¯•æ‰€æœ‰æ ¸å¿ƒç»„ä»¶
        test_all_components()
        # æ­¥éª¤2ï¼šæµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
        model, src, tgt, tgt_mask, tgt_vocab_size = test_model_forward()
        # æ­¥éª¤3ï¼šæµ‹è¯•æ¨¡å‹åå‘ä¼ æ’­ï¼ˆå¯è®­ç»ƒæ€§ï¼‰
        test_model_backward(model, src, tgt, tgt_mask, tgt_vocab_size)
        # æ‰€æœ‰æµ‹è¯•é€šè¿‡
        print("\n" + "ğŸ‰"*20)
        print("ğŸ¯ Transformer å¤ç° 100% æˆåŠŸï¼")
        print("ğŸ‰"*20)
    except AssertionError as e:
        print(f"\nâŒ å¤ç°å¤±è´¥ï¼š{e}")
    except Exception as e:
        print(f"\nâŒ å¤ç°å¤±è´¥ï¼ŒæœªçŸ¥é”™è¯¯ï¼š{e}")
